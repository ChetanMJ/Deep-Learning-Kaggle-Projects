{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import *\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import shakespeare_data as sh\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHONEME_MAP = [\n",
    "    ' ',  # \"+BLANK+\"\n",
    "    '_',  # \"+BREATH+\"\n",
    "    '+',  # \"+COUGH+\"\n",
    "    '~',  # \"+NOISE+\"\n",
    "    '!',  # \"+SMACK+\"\n",
    "    '-',  # \"+UH+\"\n",
    "    '@',  # \"+UM+\"\n",
    "    'a',  # \"AA\"\n",
    "    'A',  # \"AE\"\n",
    "    'h',  # \"AH\"\n",
    "    'o',  # \"AO\"\n",
    "    'w',  # \"AW\"\n",
    "    'y',  # \"AY\"\n",
    "    'b',  # \"B\"\n",
    "    'c',  # \"CH\"\n",
    "    'd',  # \"D\"\n",
    "    'D',  # \"DH\"\n",
    "    'e',  # \"EH\"\n",
    "    'r',  # \"ER\"\n",
    "    'E',  # \"EY\"\n",
    "    'f',  # \"F\"\n",
    "    'g',  # \"G\"\n",
    "    'H',  # \"HH\"\n",
    "    'i',  # \"IH\"\n",
    "    'I',  # \"IY\"\n",
    "    'j',  # \"JH\"\n",
    "    'k',  # \"K\"\n",
    "    'l',  # \"L\"\n",
    "    'm',  # \"M\"\n",
    "    'n',  # \"N\"\n",
    "    'G',  # \"NG\"\n",
    "    'O',  # \"OW\"\n",
    "    'Y',  # \"OY\"\n",
    "    'p',  # \"P\"\n",
    "    'R',  # \"R\"\n",
    "    's',  # \"S\"\n",
    "    'S',  # \"SH\"\n",
    "    '.',  # \"SIL\"\n",
    "    't',  # \"T\"\n",
    "    'T',  # \"TH\"\n",
    "    'u',  # \"UH\"\n",
    "    'U',  # \"UW\"\n",
    "    'v',  # \"V\"\n",
    "    'W',  # \"W\"\n",
    "    '?',  # \"Y\"\n",
    "    'z',  # \"Z\"\n",
    "    'Z',  # \"ZH\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = 'wsj0_train.npy'\n",
    "path_train_lables = 'wsj0_train_merged_labels.npy'\n",
    "path_valid = 'wsj0_dev.npy'\n",
    "path_valid_lables = 'wsj0_dev_merged_labels.npy'\n",
    "path_test = 'wsj0_test.npy'\n",
    "\n",
    "train_data = np.load(path_train, allow_pickle = True, encoding='bytes')\n",
    "train_labels = np.load(path_train_lables, allow_pickle = True, encoding= 'bytes')\n",
    "valid_data = np.load(path_valid, allow_pickle = True, encoding='bytes')\n",
    "valid_labels = np.load(path_valid_lables, allow_pickle = True, encoding= 'bytes')\n",
    "test = np.load(path_test, allow_pickle = True, encoding='bytes')\n",
    "\n",
    "class UtterancesDataset(Dataset):\n",
    "    def __init__(self,utterances, phonemes):\n",
    "        self.utterances = utterances\n",
    "        self.phonemes = phonemes\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        utterance = self.utterances[index]\n",
    "        phoneme = self.phonemes[index] + 1\n",
    "        return utterance, phoneme\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "def collate_lines(seq_list):\n",
    "    inputs,targets = zip(*seq_list) \n",
    "    input_lens = [len(seq) for seq in inputs]\n",
    "    target_lens = [len(seq) for seq in targets]\n",
    "    inputs = [torch.tensor(l) for l in inputs]   \n",
    "    inputs = pad_sequence(inputs)\n",
    "    targets = [torch.tensor(l) for l in targets]\n",
    "    targets = pad_sequence(targets,batch_first=True)\n",
    "    return inputs.to(DEVICE), targets.to(DEVICE), torch.LongTensor(input_lens).to(DEVICE), torch.LongTensor(target_lens).to(DEVICE)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36, 15,  8, 19, 23, 27, 18, 26, 32, 33,  8, 14, 40, 34, 22, 44,  8,\n",
       "       26, 22, 37, 17,  8, 41, 37, 40, 37, 22, 19,  9, 33, 43,  8, 29, 22,\n",
       "       28, 28, 30, 41, 16, 27, 12, 17,  7, 28, 14, 14, 22, 34, 16, 27, 12,\n",
       "       17,  0, 36])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = UtterancesDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=50, collate_fn = collate_lines)\n",
    "\n",
    "valid_dataset = UtterancesDataset(valid_data, valid_labels)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=10, collate_fn = collate_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1519, 50, 40])\n",
      "torch.Size([50, 148])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for inputs,targets,input_lengths,target_lengths in train_loader:\n",
    "    print(inputs.size())\n",
    "    print(targets.size())\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "    if i > 0:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,out_phonems, input_vocab_length, hidden_size, nlayers):\n",
    "        super(Model, self).__init__()\n",
    "        '''\n",
    "        self.conv1 = nn.Conv1d(in_channels = input_vocab_length, out_channels = 200, kernel_size = 9, stride=1, padding=4)\n",
    "        self.bn1 = nn.BatchNorm1d(200)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels = 200, out_channels = 400, kernel_size = 5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(400)\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        \n",
    "        ## if cnn used then chnage input size to 200\n",
    "        \n",
    "        '''\n",
    "        self.lstm = nn.LSTM(input_size = input_vocab_length,hidden_size=hidden_size,num_layers=nlayers, bidirectional=True)\n",
    "        self.output1 = nn.Linear(hidden_size * 2, 150)\n",
    "        self.output2 = nn.Linear(150, out_phonems)\n",
    "    \n",
    "    def forward(self, inputs, input_lens):\n",
    "        '''\n",
    "        inputs = inputs.transpose(0,1)\n",
    "        inputs = inputs.transpose(1,2)\n",
    "        \n",
    "        conv1_out = self.conv1(inputs)\n",
    "        conv1_bn = self.bn1(conv1_out)\n",
    "        conv1_tanh = self.tanh1(conv1_bn)\n",
    "        \n",
    "        conv2_out = self.conv2(conv1_out)\n",
    "        conv2_bn = self.bn2(conv2_out)\n",
    "        conv2_tanh = self.tanh2(conv2_bn)\n",
    "        \n",
    "        output_cnn = conv1_tanh.transpose(0,2).transpose(1,2)\n",
    "        '''\n",
    "        \n",
    "        packed_input = pack_padded_sequence(inputs, input_lens, enforce_sorted = False)\n",
    "        output_packed = self.lstm(packed_input)[0]\n",
    "        output_padded, out_lens = rnn.pad_packed_sequence(output_packed) # unpacked output (padded). Also gives you the lengths\n",
    "        out1 = self.output1(output_padded)\n",
    "        out = self.output2(out1).log_softmax(2)\n",
    "        return out, out_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoneme_list as pl\n",
    "\n",
    "phoneme_length = len(PHONEME_MAP)\n",
    "input_vocab_length = train_data[0].shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(phoneme_length, input_vocab_length, 500, 4)\n",
    "model.to(DEVICE)\n",
    "\n",
    "learningRate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_packed(model, optimizer, train_loader, val_loader, learning_Rate):\n",
    "    criterion = nn.CTCLoss() # sum instead of averaging, to take into account the different lengths\n",
    "    criterion = criterion.to(DEVICE)\n",
    "    model.train()\n",
    "    batch_id=0\n",
    "    before = time.time()\n",
    "    print(\"Training\", len(train_loader), \"number of batches\")\n",
    "    \n",
    "    optimizer.learning_rate = learning_Rate\n",
    "    \n",
    "    training_loss = 0.0\n",
    "    ntimeframes = 0\n",
    "    for inputs,targets,input_lengths,target_lengths in train_loader: # lists, presorted, preloaded on GPU\n",
    "        batch_id+=1\n",
    "        optimizer.zero_grad()\n",
    "        outputs, output_lengths = model(inputs, input_lengths)\n",
    "        loss = criterion(outputs,targets,output_lengths,target_lengths) # criterion of the concatenated output\n",
    "        training_loss = training_loss + loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ntimeframes += np.sum(np.array([len(l) for l in inputs]))\n",
    "        \n",
    "    Avg_training_loss = training_loss / len(train_loader)\n",
    "    \n",
    "    print(\"Avg Training loss\",Avg_training_loss)\n",
    "    \n",
    "    \n",
    "    val_loss = 0\n",
    "    batch_id=0\n",
    "    ntimeframes = 0\n",
    "    batch_size = len(val_loader)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    model.zero_grad()\n",
    "    for inputs,targets,input_lengths,target_lengths in val_loader:\n",
    "        ntimeframes += np.sum(np.array([len(l) for l in inputs]))\n",
    "        batch_id+=1\n",
    "        outputs, output_lengths = model(inputs, input_lengths)\n",
    "        loss = criterion(outputs,targets,output_lengths,target_lengths)\n",
    "        val_loss+=loss.item()\n",
    "        \n",
    "    val_lptf = val_loss / batch_size\n",
    "    print(\"\\nValidation Loss\", val_lptf)\n",
    "        \n",
    "    \n",
    "    return Avg_training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Training 495 number of batches\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c56dc4f121c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpoch:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mlearrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearrate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-57000262d417>\u001b[0m in \u001b[0;36mtrain_epoch_packed\u001b[0;34m(model, optimizer, train_loader, val_loader, learning_Rate)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mbatch_id\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_lengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# criterion of the concatenated output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-4b9127c89441>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, input_lens)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mpacked_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0moutput_packed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0moutput_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_packed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# unpacked output (padded). Also gives you the lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_packed\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mmax_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n\u001b[0;32m--> 525\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learrate = learningRate\n",
    "for epoch in range(15):\n",
    "    model.zero_grad()\n",
    "    print(\"\\nEpoch:\",epoch)\n",
    "    avg_loss = train_epoch_packed(model, optimizer, train_loader, train_loader, learrate)\n",
    "    learrate = learrate*0.95\n",
    "    \n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict':model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "                }, \"model_params_new\" + str(epoch) + \".tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 15\n",
      "Training 495 number of batches\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8219a2323d7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpoch:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlearrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearrate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.90\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-57000262d417>\u001b[0m in \u001b[0;36mtrain_epoch_packed\u001b[0;34m(model, optimizer, train_loader, val_loader, learning_Rate)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(5):\n",
    "    \n",
    "    epoch = e + 15\n",
    "    \n",
    "    model.zero_grad()\n",
    "    print(\"\\nEpoch:\",epoch)\n",
    "    avg_loss = train_epoch_packed(model, optimizer, train_loader, train_loader, learrate)\n",
    "    learrate = learrate*0.90\n",
    "    \n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict':model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "                }, \"model_params_new\" + str(epoch) + \".tar\")  \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(5):\n",
    "    \n",
    "    epoch = e + 20\n",
    "    \n",
    "    model.zero_grad()\n",
    "    print(\"\\nEpoch:\",epoch)\n",
    "    avg_loss = train_epoch_packed(model, optimizer, train_loader, train_loader, learrate)\n",
    "    learrate = learrate*0.85\n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict':model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "                }, \"model_params_new\" + str(epoch) + \".tar\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(3):\n",
    "    \n",
    "    epoch = e + 25\n",
    "    \n",
    "    model.zero_grad()\n",
    "    print(\"\\nEpoch:\",epoch)\n",
    "    #avg_loss = train_epoch_packed(model, optimizer, train_loader, train_loader, learrate)\n",
    "    learrate = learrate*0.05\n",
    "    \n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict':model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "                }, \"model_params_new\" + str(epoch) + \".tar\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_params_new27.tar\")\n",
    "model.cuda()\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
    "%cd ctcdecode\n",
    "!pip3 install wget\n",
    "!pip3 install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load(path_test, allow_pickle = True, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_UtterancesDataset(Dataset):\n",
    "    def __init__(self,utterances):\n",
    "        self.utterances = utterances\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        utterance = self.utterances[index]\n",
    "        return utterance\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "def collate_lines_test(seq_list):\n",
    "    inputs = seq_list\n",
    "    input_lens = [len(seq) for seq in inputs]\n",
    "    inputs = [torch.tensor(l) for l in inputs]\n",
    "    inputs = pad_sequence(inputs)\n",
    "    return inputs.to(DEVICE), torch.LongTensor(input_lens).to(DEVICE)\n",
    "    \n",
    "test_dataset = Test_UtterancesDataset(test_data)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=20, collate_fn = collate_lines_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "\n",
    "decoder = CTCBeamDecoder(['$'] * phoneme_length, beam_width=1000, log_probs_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_phonemes = []\n",
    "\n",
    "batch_size = len(test_loader)\n",
    "\n",
    "print(\"No of batches\", batch_size)\n",
    "\n",
    "b = 0\n",
    "for inputs,input_lengths in test_loader:\n",
    "    print(\"Batch: \", b)\n",
    "    b = b + 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out, out_lens = model(inputs, input_lengths)\n",
    "        \n",
    "    test_Y, _, _, test_Y_lens = decoder.decode(out.transpose(0,1), out_lens)\n",
    "    \n",
    "    batch_len = test_Y.shape[0]\n",
    "    \n",
    "    for i in range(batch_len):\n",
    "        best_seq = test_Y[i, 0, :test_Y_lens[i, 0]]\n",
    "        best_pron = ''.join(PHONEME_MAP[i] for i in best_seq)\n",
    "        out_phonemes.append(best_pron)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_np = np.array(out_phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('output.csv', out_np, delimiter=',', fmt='%s') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
