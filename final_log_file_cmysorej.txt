Approach:
Encoder: used 3 layer PBLSTM and a biLSTM as initial layer with 256 hidden size in each direction. Also added 3 dropout layers with probability of 0.2, one each for the output of pibilstm
Attention: Used the exact aproach mentioned in recitions
Decoder: used 2 lstm cells with attentions with 512 hidden size. Also, used drop out layer with probability of 0.2 for output of each lstm cell.
Batchsize: 128
Initialise: Xavier initialization
Decoding: Greedy search
No. of epochs: 22nd epochs or 21st epoch number as in log (even though i ran until 26 epochs, I had to search for the best one among the outputs for each epoch starting 20)
Teacher forcing and Gumble noise: TF and GN was avoided for initial character input to decoder while training as it going to remain same even during testing
                                  TF rate started with 0.1 and gradually increased by 5% in every subsequebt epoch until it reached a max of 0.4
Also during testing, index of '<sos>' was passed as first character. This made a difference as well
Index handling: each character index was incremented by 1 during letter to index transformation and mask indexes were handled accordingly. This made hude difference in final output.

Other aproaches which were close but not enough: batchsize - 64, 2 dropouts in encoder and 1 in decoder with 0.1 prob until epoch 29 
                                                 and increase drop out to 0.2 unntil epoch 50. With rest of the remain same as above. 
                                                 Although, loss was lower, but this took me only upto score of 12.8 which means it was overfitting. 
                                                 So, batch size increase from 64 to 128 was and dropout porb increase from start to 0.2 was helpfull.


Final log file:
Epoch:  0
/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:60: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
train_loss 3.3444778707784106
train_loss 2.8553258489128037
train_loss 2.458418285389482
train_loss 2.3424324719599774
train_loss 2.0986038367080995
train_loss 2.001934452143978
train_loss 1.9493888300376647
train_loss 1.8832825651160146
Epoch:  1
train_loss 1.880969104398269
train_loss 1.8276460407250839
train_loss 1.7348645122719857
train_loss 1.7990073567560938
train_loss 1.6040909676823099
train_loss 1.5290259737216974
train_loss 1.3742976293431157
train_loss 1.259224964735999
Epoch:  2
train_loss 1.1833989160649316
train_loss 0.9738041902141582
train_loss 0.8546082894935562
train_loss 0.7954202451884723
train_loss 0.7443933981803919
train_loss 0.6872215291129458
train_loss 0.687012698251003
train_loss 0.6276096867919012
Epoch:  3
train_loss 0.6216111565505025
train_loss 0.6168176662322321
train_loss 0.5821829196960322
train_loss 0.521454969214684
train_loss 0.5099526328983245
train_loss 0.4398380815693021
train_loss 0.4716506169361755
train_loss 0.4661292662482806
Epoch:  4
train_loss 0.44919578673183835
train_loss 0.4004854513915437
train_loss 0.4188978512307039
train_loss 0.4102697383427776
train_loss 0.36209711897263835
train_loss 0.3552851210585586
train_loss 0.4039569309860197
train_loss 0.4480787043461368
Epoch:  5
train_loss 0.33756727075792076
train_loss 0.2975072933630929
train_loss 0.32885892946331274
train_loss 0.3414303552471088
train_loss 0.3359815037549871
train_loss 0.3030334881678146
train_loss 0.36916823603621657
train_loss 0.3374336750989395
Epoch:  6
train_loss 0.29487133446959435
train_loss 0.28691617044763884
train_loss 0.28104318560697356
train_loss 0.30530097239900006
train_loss 0.27089423412235253
train_loss 0.298986171741908
train_loss 0.29782789407753935
train_loss 0.2545700103659995
Epoch:  7
train_loss 0.27094033557193214
train_loss 0.2444993818856917
train_loss 0.2638838675326041
train_loss 0.22703718324838415
train_loss 0.24337419302813984
train_loss 0.2411420160201934
train_loss 0.24811777545317007
train_loss 0.21823745900246214
Epoch:  8
train_loss 0.21849147305657732
train_loss 0.21697625650368407
train_loss 0.22270379990183245
train_loss 0.24578633364500277
train_loss 0.22103948496603623
train_loss 0.26819335730542704
train_loss 0.19344689402366147
train_loss 0.24955721902329614
Epoch:  9
train_loss 0.18527884528709634
train_loss 0.21535205765627471
train_loss 0.18125874984667498
train_loss 0.20663606931677808
train_loss 0.18122111760107829
train_loss 0.2161978306617877
train_loss 0.19319608056743615
train_loss 0.1798337778824007
Epoch:  10
train_loss 0.179986766925518
train_loss 0.13657642843691864
train_loss 0.17441431574631988
train_loss 0.20647776270825663
train_loss 0.18691824529739726
train_loss 0.2094420880142874
train_loss 0.20961391055430578
train_loss 0.18257044449730783
Epoch:  11
train_loss 0.1789083301806375
train_loss 0.2043508592100053
train_loss 0.15878207848948742
train_loss 0.176807883444583
train_loss 0.17542126666187188
train_loss 0.21702754850734327
train_loss 0.1609250982366281
train_loss 0.16654554722009476
Epoch:  12
train_loss 0.12744451389082737
train_loss 0.16099042750116604
train_loss 0.1613165475654309
train_loss 0.13623409539294037
train_loss 0.13935987708493286
train_loss 0.14613429154227398
train_loss 0.18330567804309522
train_loss 0.1730367920803249
Epoch:  13
train_loss 0.1751397388094012
train_loss 0.12900573059351392
train_loss 0.1523823722145332
train_loss 0.16170128838017322
train_loss 0.150408935546875
train_loss 0.1501401531657461
train_loss 0.1927612452651515
train_loss 0.15296440532892705
Epoch:  14
train_loss 0.1488022002351283
train_loss 0.11472265727135626
train_loss 0.1277458843271319
train_loss 0.12923548084056286
train_loss 0.14370738029407132
train_loss 0.13103356584399123
train_loss 0.1577999876755483
train_loss 0.13487928930266038
Epoch:  15
train_loss 0.10537873334598094
train_loss 0.15295484138257576
train_loss 0.11010743995011846
train_loss 0.18891487776750587
train_loss 0.13262288163347943
train_loss 0.14690802488445398
train_loss 0.12747050323146
train_loss 0.13403978839488184
Epoch:  16
train_loss 0.10147779591421678
train_loss 0.12986343354536112
train_loss 0.12866692501939384
train_loss 0.10194060630810438
train_loss 0.13471118003469437
train_loss 0.14907686164580194
train_loss 0.14256816373472572
train_loss 0.1541325260126185
Epoch:  17
train_loss 0.09325357387950493
train_loss 0.1129147968040369
train_loss 0.09976025013504505
train_loss 0.10442835205040663
train_loss 0.1504737276839308
train_loss 0.12115109146512935
train_loss 0.13130308628265464
train_loss 0.1282861796036278
Epoch:  18
train_loss 0.09633289255150519
train_loss 0.1226315402809633
train_loss 0.09927882600544632
train_loss 0.10266475475772988
train_loss 0.10923706045856543
train_loss 0.12056268072623721
train_loss 0.14867946710508012
train_loss 0.13807523136415856
Epoch:  19
train_loss 0.0851461445455586
train_loss 0.11100302486410978
train_loss 0.14199666097703273
train_loss 0.10062454782281946
train_loss 0.09188015354761646
train_loss 0.09826549371564414
train_loss 0.11372616379833504
train_loss 0.10905516983022928
Epoch:  20
train_loss 0.09520551399929093
train_loss 0.12983679862449524
train_loss 0.09858562371413225
train_loss 0.12228978359753241
train_loss 0.12571882123110562
train_loss 0.12832859041895217
train_loss 0.1081112993424023
train_loss 0.10623838753297062
Epoch:  21
train_loss 0.12180421189326894
train_loss 0.09751441313884317
train_loss 0.08921404477227328
train_loss 0.09863994855978585
train_loss 0.09923425851172345
train_loss 0.07469022781102944
train_loss 0.09978986023950942
train_loss 0.10777275037758233
Epoch:  22
train_loss 0.11718681395816387
train_loss 0.11361563970947734
train_loss 0.13753605640734234
train_loss 0.11910604309612456
train_loss 0.0917693348564345
train_loss 0.09586095834870116
train_loss 0.11587898821751451
train_loss 0.15823120108000302
Epoch:  23
train_loss 0.09391128813059145
train_loss 0.1065120347922733
train_loss 0.11395660716165677
train_loss 0.15976445480301832
train_loss 0.12337513588657639
train_loss 0.10658567466487179
train_loss 0.1277414159065079
train_loss 0.10073878622462606
Epoch:  24
train_loss 0.09984323267205819
train_loss 0.14614045516304347
train_loss 0.12222531193960195
train_loss 0.10595817766540665
train_loss 0.08724662538195486
train_loss 0.09602776181887894
train_loss 0.091318001901498
train_loss 0.10342526290841493
Epoch:  25
train_loss 0.09493141679096763
train_loss 0.12525760628306506
train_loss 0.11712618827819825
train_loss 0.10808389232888774
train_loss 0.09423120960310304
train_loss 0.1237095763506812
train_loss 0.09791205783033168
train_loss 0.14609543198652675